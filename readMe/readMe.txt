# `patent_novelty` Project

A Python pipeline for detecting **novelty**, **surprise**, and **distributional divergence** in patent texts. Combines natural language processing, statistical modeling, and unsupervised learning for analyzing innovation patterns over time.

---

## ğŸ“¦ Data Setup

### 1. Patent Compressed Data  
- **Source**: [HuggingFace HUPD Dataset](https://huggingface.co/datasets/HUPD/hupd/tree/main/data)  
- **Download**: `.tar.gz` files for selected years (2007â€“2016)

### 2. Vocabulary Files  
- **Sources**:  
  - [Technical Stopwords](https://github.com/SerhadS/TechNet/tree/master/vocabulary)  
  - [Additional Stopwords](https://github.com/SerhadS/TechNet/tree/master/additional_stopwords)  
- **Download**:
  - `technical_stopwords.txt`
  - `USPTO_stopwords_en.txt`

### 3. Citations Data  
- **Source**: [PatentsView Data Portal](https://patentsview.org/download/data-download-dictionary)  
- **Download**:  
  - `g_patent.tsv`  
  - `g_us_patent_citation.tsv`

> **Note**: Files marked with `*` in the structure below must be downloaded manually.

---

## ğŸ—‚ï¸ Project Structure

```
patent_novelty/
â”œâ”€â”€ metrics/
â”‚   â”œâ”€â”€ metrics_raw/
â”‚   â”œâ”€â”€ top10/
â”‚   â”œâ”€â”€ cd_index/
â”‚   â”œâ”€â”€ metrics/
â”‚   â””â”€â”€ metricAnalysis/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ app_pat_match/
â”‚   â”œâ”€â”€ citationsData/
â”‚   â”‚   â”œâ”€â”€ g_patent.tsv               * Download required
â”‚   â”‚   â””â”€â”€ g_us_patent_citation.tsv  * Download required
â”‚   â”œâ”€â”€ compressedData/               * .tar.gz files (2007â€“2016)
â”‚   â”œâ”€â”€ csv_clean/
â”‚   â”‚   â”œâ”€â”€ ES/
â”‚   â”‚   â”œâ”€â”€ KS/
â”‚   â”‚   â””â”€â”€ tE/
â”‚   â”œâ”€â”€ csv_raw/
â”‚   â”‚   â”œâ”€â”€ ES/
â”‚   â”‚   â”‚   â””â”€â”€ text/
â”‚   â”‚   â”œâ”€â”€ KS/
â”‚   â”‚   â””â”€â”€ tE/
â”‚   â”œâ”€â”€ jsonData/
â”‚   â””â”€â”€ vocab/
â”‚       â”œâ”€â”€ technet from github/
â”‚       â”‚   â”œâ”€â”€ vocab_github_1.tsv    * Download required
â”‚       â”‚   â””â”€â”€ vocab_github_2.tsv    * Download required
â”‚       â””â”€â”€ additional stopwords/
â”‚           â”œâ”€â”€ manual_adds.txt
â”‚           â”œâ”€â”€ technical_stopwords.txt   * Download required
â”‚           â””â”€â”€ USPTO_stopwords_en.txt    * Download required

```

\* Files marked with an asterisk must be downloaded manually.

---

## ğŸ§© Module Descriptions

### `importation.py`  
Handles data extraction and transformation from `.tar.gz` archives to JSON and CSV formats.

### `cleaning.py`  
Provides tools for token cleaning, lemmatization, and stopword filtering based on vocabularies. Uses `spaCy`, `NLTK`, and `pandas`.

### `divergences.py`  
Implements Jensen-Shannon Divergence (JSD) computations for comparing probability distributions.

### `novelty.py`  
Defines classes for novelty detection:
- **Newness**: JSD-based divergence from reference
- **Uniqueness**: Distributional shift vs prototype
- **Difference**: Deviation from neighboring distributions
- **ClusterKS**: KMeans clustering for scalable divergence analysis

### `surprise.py`  
Analyzes co-occurrence pattern shifts using PMI (Pointwise Mutual Information) and surprise scoring.

### `utils.py`  
Utility functions for:
- Vocabulary and lemmatization pipelines
- PMI construction and update
- Metric evaluation and statistical testing

---

## ğŸ“Š Key Features

- **Supports longitudinal analysis** across multiple years and IPC classes  
- **Modular design** for extensibility and experimentation  
- **Built-in statistical validation** (t-tests, correlations, etc.)  
- **Compatible with sparse/dense representations** for scalable divergence measures  
- **Batch processing** with progress tracking for large corpora  

---

## âš™ï¸ Requirements

Recommended dependencies:
- `pandas`, `numpy`, `scipy`, `nltk`, `spacy`, `tqdm`, `sklearn`, `statsmodels`, `rbo`

To install via pip:
```bash
pip install -r requirements.txt
